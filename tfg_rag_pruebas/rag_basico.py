# --- PASO 1: IMPORTAR LAS LIBRER√çAS NECESARIAS ---
print("‚û°Ô∏è Paso 1: Importando librer√≠as...")

import rdflib
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
# --- MODIFICACI√ìN: Importamos el nuevo modelo de embeddings ---
from langchain_huggingface import HuggingFaceEmbeddings

print("‚úÖ Librer√≠as importadas.")
print("-" * 30)

# --- PASO 2: CARGAR LA ONTOLOG√çA Y EXTRAER EL TEXTO LEGIBLE ---
print("‚û°Ô∏è Paso 2: Cargando la ontolog√≠a y extrayendo descripciones...")

# Crear un grafo RDF vac√≠o
g = rdflib.Graph()

# Cargar el fichero de la ontolog√≠a.
try:
    g.parse("mo_2013-07-22.n3", format="n3")
    print(f"‚úÖ Ontolog√≠a cargada con √©xito. Contiene {len(g)} tripletas.")
except Exception as e:
    print(f"‚ùå Error al cargar la ontolog√≠a: {e}")
    exit()

# Consulta SPARQL para extraer todas las clases/propiedades y sus comentarios (descripciones)
query_text = """
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT ?subject ?comment
    WHERE {
        ?subject rdfs:comment ?comment .
    }
"""
results = g.query(query_text)

# Crear una lista de "documentos". Cada documento ser√° un texto que describe un concepto.
documents = []
for row in results:
    # Filtramos los comentarios para que tengan una longitud m√≠nima, evitando ruido.
    if len(row.comment) > 20:
        doc_text = f"Concepto URI: {row.subject}\nDescripci√≥n: {row.comment}"
        documents.append(doc_text)

print(f"‚úÖ Se han extra√≠do {len(documents)} descripciones de la ontolog√≠a.")
print("-" * 30)

# --- PASO 3: CONSTRUIR LA CADENA RAG ---
print("‚û°Ô∏è Paso 3: Construyendo la cadena RAG con LangChain...")

# --- MODIFICACI√ìN: Cambiamos el motor de embeddings ---
# En lugar de usar Llama3 para los embeddings (que no es su especialidad),
# usamos un modelo dise√±ado espec√≠ficamente para la b√∫squeda sem√°ntica multiling√ºe.
# La primera vez que ejecutes esto, tardar√° un poco en descargar el modelo (~500MB).
print("‚úÖ Configurando embeddings con Sentence Transformers (esto puede tardar la primera vez)...")
model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
embeddings = HuggingFaceEmbeddings(model_name=model_name)
print("‚úÖ Embeddings listos.")


# 3.2 - Vector Store: La base de datos que almacena los vectores.
# Chroma.from_texts crea una base de datos vectorial en memoria a partir de nuestra lista de documentos.
vectorstore = Chroma.from_texts(texts=documents, embedding=embeddings)

# 3.3 - Retriever: El componente que busca en la base de datos vectorial.
# Dado un texto, encontrar√° los documentos (vectores) m√°s similares.
retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) # k=3 significa que traer√° los 3 resultados m√°s relevantes.
print("‚úÖ Retriever configurado.")

# 3.4 - LLM: El modelo de lenguaje que generar√° la respuesta.
# Seguimos usando llama3 para la generaci√≥n, ¬°es excelente para eso!
llm = ChatOllama(model="llama3")

# 3.5 - Prompt Template: La plantilla que le daremos al LLM.
# Le damos instrucciones claras para que use solo el contexto que le proporcionamos.
template = """
Act√∫a como un asistente experto en la 'Music Ontology'. Responde a la pregunta del usuario bas√°ndote √öNICAMENTE en el siguiente contexto extra√≠do de la ontolog√≠a. Si la informaci√≥n no est√° en el contexto, di que no lo sabes.

CONTEXTO:
{context}

PREGUNTA:
{question}

RESPUESTA:
"""
prompt = ChatPromptTemplate.from_template(template)
print("‚úÖ Plantilla de prompt creada.")

# 3.6 - Cadena RAG (RAG Chain): Unimos todas las piezas.
# Esta es la parte m√°s potente de LangChain. Define el flujo de datos.
rag_chain = (
    # El diccionario de entrada pasa el contexto (buscado por el retriever) y la pregunta original.
    {"context": retriever, "question": RunnablePassthrough()}
    # Pasamos el diccionario al prompt para que lo formatee.
    | prompt
    # El prompt formateado se pasa al LLM.
    | llm
    # La salida del LLM se convierte a un string simple.
    | StrOutputParser()
)

print("‚úÖ Cadena RAG construida y lista para usarse.")
print("-" * 30)

# --- PASO 4: HACER PREGUNTAS AL SISTEMA ---
print("‚û°Ô∏è Paso 4: ¬°Haciendo preguntas! (Escribe 'salir' para terminar)")

# Bucle interactivo para que puedas hacer varias preguntas.
while True:
    user_question = input("\nü§î ¬øQu√© quieres saber sobre la Music Ontology?: ")
    if user_question.lower() == 'salir':
        break
    
    print("\nüí¨ Pensando...")
    # Invocamos la cadena con la pregunta del usuario.
    # El flujo definido en el paso 3.6 se ejecuta autom√°ticamente.
    response = rag_chain.invoke(user_question)
    
    print("\n‚úÖ Respuesta del LLM:")
    print(response)

print("\nüëã ¬°Hasta luego!")