# rag_basico.py (Versi√≥n con Arquitectura RAG de 3 Etapas - Selecci√≥n de Ontolog√≠as)
print("Paso 1: Importando librer√≠as...")

import os
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings

print("Librer√≠as importadas.")
print("-" * 30)

# PASO 2: CONFIGURACI√ìN INICIAL
print("Paso 2: Configurando componentes...")

# 2.1 - Embeddings
print("Configurando embeddings con Sentence Transformers...")
model_name = "BAAI/bge-m3"
embeddings = HuggingFaceEmbeddings(model_name=model_name)
print(f"Embeddings ({model_name}) listos.")

# 2.2 - Vector Store
persist_directory = "tfg_rag_pruebas/chroma_db"
print(f"Cargando la base de datos vectorial desde '{persist_directory}'...")

if not os.path.exists(persist_directory):
    print(f"\n--- ERROR ---")
    print(f"No se encuentra el directorio de la base de datos: '{persist_directory}'")
    exit()

try:
    vectorstore = Chroma(
        persist_directory=persist_directory, 
        embedding_function=embeddings
    )
    
    # Verificaci√≥n
    db_count = vectorstore._collection.count()
    if db_count == 0:
        print(f"\n--- ERROR ---")
        print("La base de datos se ha cargado pero est√° VAC√çA (0 documentos).")
        print("Por favor, ejecuta 'chroma_db.py' primero.")
        exit()
        
    print(f"Base de datos vectorial cargada con √©xito. Contiene {db_count} documentos.")

except Exception as e:
    print(f"\n--- ERROR ---")
    print(f"No se pudo cargar la base de datos desde '{persist_directory}'.")
    print(f"Detalle del error: {e}")
    exit()

# 2.3 - LLM
print("Configurando LLM (Llama3 v√≠a Ollama)...")
llm = ChatOllama(model="llama3", temperature=0.0)
print("LLM configurado.")

print("-" * 30)

# PASO 3: DEFINIR PROMPTS PARA LAS 3 ETAPAS

# ETAPA 1: Extracci√≥n de Conceptos
extraction_template = """
Eres un experto en an√°lisis sem√°ntico. Tu tarea es extraer √∫nicamente las palabras clave de b√∫squeda de la petici√≥n del usuario.

Petici√≥n del usuario: **{user_request}**

Instrucciones:
1. Identifica los conceptos principales (Clases) y las relaciones (Propiedades) que el usuario necesita modelar.
2. Extrae SOLO las palabras clave relevantes para buscar en una base de datos de ontolog√≠as.
3. Responde con una lista concisa de t√©rminos de b√∫squeda separados por comas.
4. NO proporciones explicaciones, solo las palabras clave.

Ejemplo de respuesta: "Author, Paper, Person, Publication, writes, hasAuthor"

Palabras clave de b√∫squeda:
"""

extraction_prompt = ChatPromptTemplate.from_template(extraction_template)
extraction_chain = extraction_prompt | llm | StrOutputParser()

# ETAPA 3: Selecci√≥n y Decisi√≥n
selection_template = """
Eres un experto en selecci√≥n de ontolog√≠as. Tu tarea es analizar los resultados de b√∫squeda y recomendar UNA √∫nica ontolog√≠a.

Petici√≥n original del usuario: **{user_request}**

Resultados de b√∫squeda con sus fuentes:
---
{context_with_sources}
---

Instrucciones:
1. Analiza qu√© ontolog√≠a (Fuente) contiene M√ÅS conceptos relevantes para la petici√≥n del usuario.
2. Cuenta el n√∫mero de conceptos √∫tiles por cada ontolog√≠a.
3. Eval√∫a la calidad y completitud de los conceptos encontrados.
4. Recomienda UNA √∫nica ontolog√≠a que mejor cubra los requisitos del usuario.
5. Explica brevemente por qu√© esa ontolog√≠a es la mejor opci√≥n (menciona qu√© conceptos clave contiene).

Tu respuesta debe tener este formato:
**ONTOLOG√çA RECOMENDADA:** [nombre_del_archivo]
**RAZ√ìN:** [explicaci√≥n breve de 2-3 l√≠neas]
**CONCEPTOS CLAVE ENCONTRADOS:** [lista de 3-5 conceptos principales]

Respuesta:
"""

selection_prompt = ChatPromptTemplate.from_template(selection_template)
selection_chain = selection_prompt | llm | StrOutputParser()

print("Prompts de las 3 etapas configurados.")
print("-" * 30)

# PASO 4: BUCLE PRINCIPAL DE CONSULTAS
print("Paso 4: Sistema de Selecci√≥n de Ontolog√≠as listo.")
print("\n¬°Bienvenido al Sistema de Selecci√≥n de Ontolog√≠as!")
print("Describe qu√© datos necesitas modelar y te recomendar√© la mejor ontolog√≠a.")
print("-" * 60)

while True:
    user_request = input("\n¬øQu√© necesitas modelar? (o 'salir' para terminar): ")
    
    if user_request.lower() == 'salir':
        break
    
    print("\n" + "=" * 60)
    print("ETAPA 1: EXTRACCI√ìN DE CONCEPTOS")
    print("=" * 60)
    
    try:
        # ETAPA 1: Extraer palabras clave con el LLM
        print(" Analizando tu petici√≥n para extraer conceptos clave...")
        search_query = extraction_chain.invoke({"user_request": user_request})
        print(f"\n Palabras clave extra√≠das:\n   {search_query}")
        
    except Exception as e:
        print(f"\n ERROR en la extracci√≥n de conceptos: {e}")
        continue
    
    print("\n" + "=" * 60)
    print("ETAPA 2: B√öSQUEDA EN BASE DE DATOS")
    print("=" * 60)
    
    try:
        # ETAPA 2: Buscar en el vectorstore usando las palabras clave
        print(f" Buscando conceptos relacionados con: '{search_query}'...")
        retrieved_docs = vectorstore.max_marginal_relevance_search(search_query, k=15, fetch_k=50)
        
        if not retrieved_docs:
            print("\n No se encontraron resultados relevantes.")
            continue
        
        print(f" Se encontraron {len(retrieved_docs)} documentos relevantes.")
        
        # Construir contexto con fuentes
        context_with_sources = []
        source_count = {}
        
        for i, doc in enumerate(retrieved_docs):
            source = doc.metadata.get('source', 'Fuente desconocida')
            
            # Contar documentos por fuente
            source_count[source] = source_count.get(source, 0) + 1
            
            # Formatear el documento con su fuente
            content_preview = doc.page_content[:200].replace('\n', ' ')
            formatted_doc = f"- [Fuente: {source}] {content_preview}..."
            context_with_sources.append(formatted_doc)
        
        context_str = "\n".join(context_with_sources)
        
        # Mostrar estad√≠sticas de fuentes
        print("\n Distribuci√≥n de resultados por ontolog√≠a:")
        for source, count in sorted(source_count.items(), key=lambda x: x[1], reverse=True):
            print(f"   ‚Ä¢ {source}: {count} documentos")
        
    except Exception as e:
        print(f"\n ERROR en la b√∫squeda: {e}")
        continue
    
    print("\n" + "=" * 60)
    print("ETAPA 3: SELECCI√ìN DE ONTOLOG√çA")
    print("=" * 60)
    
    try:
        # ETAPA 3: Analizar y seleccionar la mejor ontolog√≠a
        print(" Analizando resultados para seleccionar la mejor ontolog√≠a...")
        
        recommendation = selection_chain.invoke({
            "user_request": user_request,
            "context_with_sources": context_str
        })
        
        print("\n" + "üéØ" * 30)
        print(recommendation)
        print("üéØ" * 30)
        
    except Exception as e:
        print(f"\n ERROR en la selecci√≥n: {e}")
        continue

print("\n¬°Hasta luego! ")