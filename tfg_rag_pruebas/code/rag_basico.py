import os
import time
import json
import re
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from sentence_transformers import CrossEncoder  # <--- NUEVA IMPORTACI칍N

# CONFIGURACI칍N
# Construir ruta absoluta din치mica: carpeta 'chroma_db' hermana de la carpeta 'code'
current_dir = os.path.dirname(os.path.abspath(__file__)) # .../tfg_rag_pruebas/code
project_root = os.path.dirname(current_dir)              # .../tfg_rag_pruebas
PERSIST_DIRECTORY = os.path.join(project_root, "chroma_db")
EMBEDDING_MODEL = "BAAI/bge-m3"
LLM_MODEL = "llama3"
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2" # <--- MODELO RERANKER

class OntologyRecommender:
    def __init__(self):
        print("Iniciando sistema RAG con B칰squeda H칤brida + Cross-Encoder Reranking...")
        
        # 1. Embeddings y Vector Store
        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        
        if not os.path.exists(PERSIST_DIRECTORY):
            raise FileNotFoundError(f"No se encuentra la BD en {PERSIST_DIRECTORY}")
            
        self.vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY, 
            embedding_function=self.embeddings
        )
        
        # 2. Cargar Cross-Encoder (Re-ranker)
        # Este modelo es mucho m치s preciso que un LLM para ordenar relevancia
        print(f" - Cargando Reranker ({RERANKER_MODEL})...")
        self.reranker = CrossEncoder(RERANKER_MODEL)

        # 3. Inicializar Retrievers
        self._setup_retrievers()
        
        # 4. Inicializar LLM y Cadenas
        self.llm = ChatOllama(model=LLM_MODEL, temperature=0.0)
        self._setup_chains()
        
        # --- WARMUP AUTOM츼TICO ---
        self._warmup_system()
        print("Sistema RAG H칤brido listo y optimizado.")

    def _warmup_system(self):
        """Ejecuta una inferencia dummy para cargar modelos en VRAM"""
        print("   游댠 Ejecutando Warmup (Cargando modelos en GPU)...")
        try:
            # 1. Calentar Embeddings
            self.embeddings.embed_query("warmup query")
            
            # 2. Calentar Reranker
            self.reranker.predict([["test query", "test document content"]])

            # 3. Calentar LLM
            self.llm.invoke("Ready?")
            print("   游댠 Modelos cargados.")
        except Exception as e:
            print(f"   丘멆잺 Error en Warmup (no cr칤tico): {e}")

    def _setup_retrievers(self):
        """Configura el sistema de recuperaci칩n h칤brida"""
        print(" - Construyendo 칤ndice BM25 (esto puede tardar unos segundos)...")
        
        try:
            collection_data = self.vectorstore.get() 
            texts = collection_data['documents']
            metadatas = collection_data['metadatas']
            
            docs = [
                Document(page_content=t, metadata=m) 
                for t, m in zip(texts, metadatas)
            ]
        except Exception as e:
            print(f"Error cargando docs para BM25: {e}")
            docs = []

        if not docs:
            raise ValueError("La base de datos Chroma parece vac칤a o no se pudo leer para BM25.")

        self.bm25_retriever = BM25Retriever.from_documents(docs)
        self.bm25_retriever.k = 40

        self.chroma_retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 25}
        )

    def _hybrid_retrieve(self, query, k=40):
        """
        Recuperaci칩n h칤brida: Dense (Chroma) + Sparse (BM25)
        """
        self.bm25_retriever.k = k 
        self.chroma_retriever.search_kwargs["k"] = k 

        dense_docs = self.chroma_retriever.invoke(query)
        sparse_docs = self.bm25_retriever.invoke(query)
        
        seen = set()
        combined = []

        max_len = max(len(dense_docs), len(sparse_docs))
        for i in range(max_len):
            if i < len(dense_docs):
                d = dense_docs[i]
                uid = (d.page_content, str(d.metadata))
                if uid not in seen:
                    seen.add(uid)
                    combined.append(d)
            
            if i < len(sparse_docs):
                d = sparse_docs[i]
                uid = (d.page_content, str(d.metadata))
                if uid not in seen:
                    seen.add(uid)
                    combined.append(d)
            
            if len(combined) >= k:
                break

        return combined

    def _setup_chains(self):
        # 1. EXTRACCI칍N (Igual que antes)
        extract_tmpl = """
        Act칰a como un termin칩logo experto en Web Sem치ntica (OWL/RDF).
        Analiza la petici칩n del usuario: "{user_request}"
        
        Genera una lista de b칰squeda optimizada siguiendo estos pasos:
        1. **Conceptos Nucleares:** Extrae los sustantivos y verbos t칠cnicos principales.
        2. **Normalizaci칩n Ontol칩gica:** A침ade los equivalentes formales m치s probables en ontolog칤as est치ndar (ej: si dice "tipo", a침ade "Category", "Class", "Type").
        3. **Sin칩nimos T칠cnicos:** Incluye t칠rminos alternativos precisos.
        
        Respuesta: Solo la lista de t칠rminos separada por comas (en ingl칠s).
        """
        self.extraction_chain = ChatPromptTemplate.from_template(extract_tmpl) | self.llm | StrOutputParser()

        # NOTA: HE ELIMINADO filter_chain PORQUE AHORA USAMOS CROSS-ENCODER

        # 2. DECISI칍N FINAL (Igual que antes pero procesar치 mejor contexto)
        selection_tmpl = """
        Act칰a como un Arquitecto de Ontolog칤as Senior. Tu decisi칩n debe basarse puramente en la l칩gica de dise침o de sistemas y la evidencia del texto.
        
        PETICI칍N USUARIO: "{user_request}"
        CANDIDATOS RECUPERADOS (Top relevantes tras an치lisis profundo):
        {filtered_context}
        
        ALGORITMO DE DECISI칍N:
        
        PASO 1: An치lisis de Especificidad.
        - 쯈uery gen칠rica ("qu칠 es un proceso") o espec칤fica de nicho ("sensores de riego")?
        
        PASO 2: An치lisis de Intencionalidad (CR칈TICO).
        - **Intenci칩n Normativa:** 쯀mplica reglas, restricciones, "Must", l칩gica? -> Busca ontolog칤as pesadas/definitorias.
        - **Intenci칩n Descriptiva:** 쯉olo busca etiquetar o metadatos? -> Prefiere vocabularios ligeros (Principio de Parsimonia).
        
        PASO 3: Selecci칩n Final.
        - Elige el archivo que mejor se alinee con el Nivel y la Intenci칩n.
        
        SALIDA (JSON estricto):
        {{
            "RAZONAMIENTO": "Explica brevemente la distinci칩n entre Intenci칩n Normativa vs Descriptiva y la elecci칩n.",
            "ONTOLOG칈A_RECOMENDADA": "nombre_archivo.ext"
        }}
        """
        self.selection_chain = ChatPromptTemplate.from_template(selection_tmpl) | self.llm | StrOutputParser()

    def run_pipeline(self, user_request, initial_k=100):
        """
        Pipeline optimizado:
        1. Query Expansion
        2. Broad Retrieval (k=100) -> Para maximizar Recall
        3. Cross-Encoder Reranking -> Para maximizar Precision
        4. LLM Selection (Top 10) -> Para razonamiento final
        """
        start_time = time.time()
        print(f"--- Inicio Pipeline: {user_request[:50]}... ---")
        
        # 1. Extracci칩n
        try: keywords = self.extraction_chain.invoke({"user_request": user_request})
        except: keywords = user_request
        print(f"Keywords: {keywords}")

        # 2. Retrieval H칈BRIDO AMPLIO (k=100)
        # Traemos muchos documentos para evitar que se nos escape el bueno
        raw_docs = self._hybrid_retrieve(keywords, k=initial_k)
        print(f"Retrieval Broad: {len(raw_docs)} docs candidatos.")
        
        # 3. RE-RANKING CON CROSS-ENCODER (El paso cr칤tico)
        print("Ejecutando Cross-Encoder Re-ranking...")
        if raw_docs:
            # Preparamos pares [Query, Doc Content]
            # Limitamos contenido a 500 chars para velocidad del reranker
            doc_contents = [d.page_content[:500] for d in raw_docs]
            pairs = [[user_request, content] for content in doc_contents]
            
            # Predecimos scores de similitud
            scores = self.reranker.predict(pairs)
            
            # Combinamos doc con score y ordenamos
            scored_docs = list(zip(raw_docs, scores))
            scored_docs_sorted = sorted(scored_docs, key=lambda x: x[1], reverse=True)
            
            # Cortamos el Top 10 (High Precision)
            top_k_reranked = 10
            final_docs = [doc for doc, score in scored_docs_sorted[:top_k_reranked]]
            
            top_score = scored_docs_sorted[0][1]
            print(f"Top {top_k_reranked} seleccionados (Score m치x: {top_score:.4f})")
        else:
            final_docs = []
            print("丘멆잺 No se recuperaron documentos en la fase inicial.")

        # 4. Preparar Contexto para el LLM
        context_lines = []
        for d in final_docs:
            src = d.metadata.get('source', 'unknown')
            otype = d.metadata.get('ontology_type', '?')
            # Podemos dar m치s contexto (600 chars) porque son pocos documentos
            content = d.page_content[:600].replace('\n', ' ')
            context_lines.append(f"- FILE: {src} [TYPE: {otype}] | CONTENT: {content}...")
            
        context_str = "\n".join(context_lines)

        # 5. Generaci칩n Final (CoT)
        print("Generando decisi칩n final con LLM...")
        decision_text = self.selection_chain.invoke({
            "user_request": user_request,
            "filtered_context": context_str
        })

        total_time = time.time() - start_time
        print(f"--- Fin Pipeline ({total_time:.2f}s) ---")

        return {
            "query": user_request,
            "keywords": keywords,
            # Devolvemos los filtrados por el Reranker para evaluar el Recall real
            "unique_retrieved_sources": list(set([d.metadata.get('source') for d in final_docs])),
            "llm_response": decision_text,
            "execution_time": total_time
        }

if __name__ == "__main__":
    rag = OntologyRecommender()
    while True:
        q = input("\nConsulta ('salir'): ")
        if q == 'salir': break
        try:
            res = rag.run_pipeline(q)
            print(f"Filtrados (Top-10 Reranked): {res['unique_retrieved_sources']}")
            print(f"Respuesta:\n{res['llm_response']}")
        except Exception as e:
            print(f"Error: {e}")