import os
import time
import json
import re
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document



# CONFIGURACIN
# Construir ruta absoluta din谩mica: carpeta 'chroma_db' hermana de la carpeta 'code'
current_dir = os.path.dirname(os.path.abspath(__file__)) # .../tfg_rag_pruebas/code
project_root = os.path.dirname(current_dir)              # .../tfg_rag_pruebas
PERSIST_DIRECTORY = os.path.join(project_root, "chroma_db")
EMBEDDING_MODEL = "BAAI/bge-m3"
LLM_MODEL = "llama3"

class OntologyRecommender:
    # En rag_basico.py, dentro de la clase OntologyRecommender

    def __init__(self):
        print("Iniciando sistema RAG con B煤squeda H铆brida (Dense + Sparse)...")
        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        
        if not os.path.exists(PERSIST_DIRECTORY):
            raise FileNotFoundError(f"No se encuentra la BD en {PERSIST_DIRECTORY}")
            
        # 1. Cargar Base Vectorial (Chroma)
        self.vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY, 
            embedding_function=self.embeddings
        )
        
        # 2. Inicializar Retrievers
        self._setup_retrievers()
        
        # 3. Inicializar LLM y Cadenas
        self.llm = ChatOllama(model=LLM_MODEL, temperature=0.0)
        self._setup_chains()
        
        # --- NUEVO: WARMUP AUTOMTICO ---
        self._warmup_system()
        # --------------------------------
        
        print("Sistema RAG H铆brido listo y caliente (Warmup completado).")

    def _warmup_system(self):
        """Ejecuta una inferencia dummy para cargar modelos en VRAM"""
        print("    Ejecutando Warmup (Cargando modelos en GPU)...")
        try:
            # 1. Calentar Embeddings
            self.embeddings.embed_query("warmup query")
            
            # 2. Calentar LLM (Forzamos una generaci贸n corta)
            # Usamos una invoke directa simple para despertar a Ollama
            self.llm.invoke("Hello, are you ready?")
            print("    Modelos cargados.")
        except Exception as e:
            print(f"   锔 Error en Warmup (no cr铆tico): {e}")

    def _setup_retrievers(self):
        """Configura el sistema de recuperaci贸n h铆brida"""
        print(" - Construyendo 铆ndice BM25 (esto puede tardar unos segundos)...")
        
        # A. Recuperar documentos crudos de Chroma para indexar con BM25
        # NOTA: Chroma guarda los textos, necesitamos sacarlos para que BM25 los procese.
        try:
            # Obtenemos todos los documentos de la colecci贸n
            collection_data = self.vectorstore.get() 
            texts = collection_data['documents']
            metadatas = collection_data['metadatas']
            
            # Reconstruimos objetos Document para LangChain
            docs = [
                Document(page_content=t, metadata=m) 
                for t, m in zip(texts, metadatas)
            ]
        except Exception as e:
            print(f"Error cargando docs para BM25: {e}")
            docs = []

        if not docs:
            raise ValueError("La base de datos Chroma parece vac铆a o no se pudo leer para BM25.")

        # B. Retriever disperso (Keyword Search - BM25)
        self.bm25_retriever = BM25Retriever.from_documents(docs)
        self.bm25_retriever.k = 40  # Coincidir con el k del vectorial

        # C. Retriever denso (Semantic Search - Chroma)
        self.chroma_retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 25}
        )


    def _hybrid_retrieve(self, query, k=40):
        """
        Recuperaci贸n h铆brida: Dense (Chroma) + Sparse (BM25)
        Combina resultados priorizando sem谩ntica pero manteniendo keywords exactas.
        """
        # Recuperaci贸n
        dense_docs = self.chroma_retriever.invoke(query)
        sparse_docs = self.bm25_retriever.invoke(query)

        # Limitar a k resultados
        dense_docs = dense_docs[:k]
        sparse_docs = sparse_docs[:k]

        seen = set()
        combined = []

        for d in dense_docs + sparse_docs:
            uid = (d.page_content, str(d.metadata))
            if uid not in seen:
                seen.add(uid)
                combined.append(d)
            if len(combined) >= k:
                break

        return combined


    def _setup_chains(self):
        # 1. EXTRACCIN Y EXPANSIN (Query Expansion - AGNSTICO)
        # Objetivo: Enriquecer vocabulario sin asumir el dominio.
        extract_tmpl = """
        Act煤a como un termin贸logo experto en Web Sem谩ntica (OWL/RDF).
        Analiza la petici贸n del usuario: "{user_request}"
        
        Genera una lista de b煤squeda optimizada siguiendo estos pasos:
        1. **Conceptos Nucleares:** Extrae los sustantivos y verbos t茅cnicos principales.
        2. **Normalizaci贸n Ontol贸gica:** A帽ade los equivalentes formales m谩s probables en ontolog铆as est谩ndar (ej: si dice "tipo", a帽ade "Category", "Class", "Type").
        3. **Sin贸nimos T茅cnicos:** Incluye t茅rminos alternativos precisos, pero SOLO si son comunes en modelado de conocimiento.
        
        Respuesta: Solo la lista de t茅rminos separada por comas (en ingl茅s).
        """
        self.extraction_chain = ChatPromptTemplate.from_template(extract_tmpl) | self.llm | StrOutputParser()

        # 2. FILTRADO (Se mantiene igual de robusto)
        filter_tmpl = """
        Eres un filtro de relevancia sem谩ntica para ontolog铆as.
        
        INPUT:
        - Query: "{user_request}"
        - Candidatos: Lista de fragmentos de texto (RDF/OWL).

        TAREA:
        Identifica qu茅 archivos contienen definiciones ontol贸gicas (Clases, Propiedades) relacionadas con los conceptos de la query.
        
        CRITERIOS DE SELECCIN (AGNSTICOS):
        1. **Presencia de Definiciones:** Busca si el t茅rmino aparece como sujeto de definici贸n (ej: "X a owl:Class", "X a rdf:Property") y no solo como comentario.
        2. **Coincidencia Sem谩ntica:** Si la query pide "X" y el archivo define "X" o un sin贸nimo t茅cnico directo, selecci贸nalo.
        3. **Ignora el dominio:** No importa si es medicina, construcci贸n o leyes. Si define el t茅rmino, es relevante.

        SALIDA (JSON VLIDO):
        {{
            "relevant_sources": ["archivo1.ext", "archivo2.ext"]
        }}
        """
        self.filter_chain = ChatPromptTemplate.from_template(filter_tmpl) | self.llm | StrOutputParser()

        # 3. DECISIN FINAL (RAZONAMIENTO ESTRUCTURADO - SIN SESGOS)
        # ESTA ES LA VERSIN MEJORADA CON CRITERIO DE INTENCIONALIDAD FUNCIONAL
        selection_tmpl = """
        Act煤a como un Arquitecto de Ontolog铆as Senior. Tu decisi贸n debe basarse puramente en la l贸gica de dise帽o de sistemas y la evidencia del texto.
        
        PETICIN USUARIO: "{user_request}"
        CANDIDATOS RECUPERADOS (con metadatos de tipo):
        {filtered_context}
        
        ALGORITMO DE DECISIN (Ejecuta esto paso a paso):
        
        PASO 1: An谩lisis de Especificidad de la Query.
        - 驴La query solicita conceptos fundamentales/gen茅ricos (ej: "qu茅 es un proceso", "definir espacio")? -> **Nivel: GENRICO**.
        - 驴La query solicita conceptos aplicados a un nicho (ej: "sensores de riego agr铆cola", "vigas de acero reforzado")? -> **Nivel: ESPECFICO**.
        
        PASO 2: An谩lisis de Carga L贸gica e Intencionalidad (CRTICO).
        - Analiza los verbos y sustantivos de la query para determinar la complejidad funcional:
          A. **Intenci贸n Normativa/L贸gica (Compleja):** 驴La query implica reglas, restricciones, obligaciones, permisos, l贸gica condicional o comportamiento din谩mico? (Palabras clave agn贸sticas: "Constraint", "Rule", "Must", "Duty", "Function", "Process").
          B. **Intenci贸n Descriptiva/Est谩tica (Simple):** 驴La query solo busca etiquetar, anotar o describir atributos est谩ticos de un recurso? (Palabras clave agn贸sticas: "Title", "Label", "Tag", "Creator", "Metadata").
        
        PASO 3: Evaluaci贸n de Cobertura y Definici贸n.
        - Revisa el contenido de texto de cada candidato.
        - Si (A) Intenci贸n Normativa: Descarta vocabularios ligeros de anotaci贸n (aunque contengan la palabra clave) y busca ontolog铆as que definan Clases para modelar la regla/acci贸n.
        - Si (B) Intenci贸n Descriptiva: Aplica el Principio de Parsimonia. Prefiere vocabularios ligeros y est谩ndar sobre modelos complejos que matar铆an moscas a ca帽onazos.
        
        PASO 4: Selecci贸n Final.
        - Elige el archivo que mejor se alinee con el Nivel (Gen茅rico/Espec铆fico) y la Intenci贸n (Normativa/Descriptiva).
        
        SALIDA (JSON estricto):
        {{
            "RAZONAMIENTO": "Explica brevemente la distinci贸n entre Intenci贸n Normativa vs Descriptiva detectada y por qu茅 el archivo elegido es el adecuado.",
            "ONTOLOGA_RECOMENDADA": "nombre_archivo.ext"
        }}
        """
        self.selection_chain = ChatPromptTemplate.from_template(selection_tmpl) | self.llm | StrOutputParser()

    def _extract_json_from_text(self, text):
        """Extrae y parsea el primer bloque JSON encontrado en un texto."""
        try:
            # Buscar el primer bloque { ... } (incluso multilinea)
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                return json.loads(match.group(0))
            # Si no encuentra patr贸n, intenta parsear todo el texto
            return json.loads(text)
        except:
            return None

    def run_pipeline(self, user_request, initial_k=25): # Default a 25
        start_time = time.time()
        print(f"--- Inicio Pipeline: {user_request[:50]}... ---")
        
        # 1. Extracci贸n
        try: keywords = self.extraction_chain.invoke({"user_request": user_request})
        except: keywords = user_request
        print(f"Keywords: {keywords}")

        # 2. Retrieval HBRIDO
        raw_docs = self._hybrid_retrieve(keywords, k=initial_k)
        print(f"Retrieval: {len(raw_docs)} docs recuperados.")
        
        # 3. Preparar contexto para FILTRADO (OPTIMIZACIN CRTICA)
        doc_summaries = []
        for d in raw_docs:
            src = d.metadata.get('source', 'unknown')
            otype = d.metadata.get('ontology_type', '?')
            # REDUCCIN: 450 chars -> 180 chars. 
            # Suficiente para ver la definici贸n, ahorra 60% de c贸mputo.
            content = d.page_content[:180].replace('\n', ' ') 
            doc_summaries.append(f"- ID: {src} [{otype}] | TEXT: {content}...")
        
        doc_list_str = "\n".join(doc_summaries)
        
        # 3. Re-ranking (Filtrado con LLM)
        print("Ejecutando Filtro LLM (esto es lo que tardaba)...")
        relevant_files = []
        try:
            # Ahora el prompt es mucho m谩s ligero (~1.5k tokens vs 5k tokens)
            raw_filter_output = self.filter_chain.invoke({
                "user_request": user_request,
                "context_list": doc_list_str
            })
            parsed_json = self._extract_json_from_text(raw_filter_output)
            
            if parsed_json and "relevant_sources" in parsed_json:
                relevant_files = parsed_json["relevant_sources"]
            else:
                relevant_files = [d.metadata.get('source') for d in raw_docs[:10]]
        except Exception as e:
            print(f"Fallback filtro: {e}")
            relevant_files = [d.metadata.get('source') for d in raw_docs[:10]]

        if not isinstance(relevant_files, list): relevant_files = []
        print(f"Docs tras filtro: {len(relevant_files)}")

        # 4. Reconstrucci贸n Contexto (Para la decisi贸n final S damos m谩s texto)
        final_docs = [d for d in raw_docs if d.metadata.get('source') in relevant_files]
        if not final_docs: final_docs = raw_docs[:5] 

        context_lines = []
        for d in final_docs:
            src = d.metadata.get('source')
            otype = d.metadata.get('ontology_type', 'UNKNOWN')
            # Aqu铆 mantenemos 450 o incluso m谩s, porque ya son pocos documentos (top 3-5)
            content = d.page_content[:500].replace('\n', ' ')
            context_lines.append(f"- FILE: {src} [TYPE: {otype}] | CONTENT: {content}...")
            
        context_str = "\n".join(context_lines)

        # 5. Generaci贸n
        print("Generando decisi贸n final...")
        decision_text = self.selection_chain.invoke({
            "user_request": user_request,
            "filtered_context": context_str
        })

        total_time = time.time() - start_time
        print(f"--- Fin Pipeline ({total_time:.2f}s) ---")

        return {
            "query": user_request,
            "keywords": keywords,
            "unique_retrieved_sources": list(set([d.metadata.get('source') for d in final_docs])),
            "llm_response": decision_text,
            "execution_time": total_time
        }

if __name__ == "__main__":
    rag = OntologyRecommender()
    while True:
        q = input("\nConsulta ('salir'): ")
        if q == 'salir': break
        res = rag.run_pipeline(q)
        print(f"Filtrados: {res['unique_retrieved_sources']}")
        print(f"Respuesta:\n{res['llm_response']}")